{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbi:hide_in\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LassoLars,lars_path,LassoLarsCV\n",
    "from sklearn import datasets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider\n",
    "from ipywidgets.embed import embed_data\n",
    "import nbinteract as nbi\n",
    "from ipywidgets import interact, interactive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbi:hide_in\n",
    "def LarsLasso(Distribution,noise,n_samples,LogAlpha,checkbox1):\n",
    "#     Noise = Noise/100\n",
    "    n_samples = int(n_samples)\n",
    "    centers = 5\n",
    "    Alpha = np.exp(LogAlpha)\n",
    "    seed = 3\n",
    "    np.random.seed(seed)\n",
    "    if Distribution == \"Linear\":\n",
    "        X_mn = np.linspace(0, 20, n_samples)\n",
    "        m = np.random.random_sample() * 25\n",
    "        c = np.random.random_sample() * 25\n",
    "        y_mn = m*X_mn + c + np.random.rand(*X_mn.shape) * noise\n",
    "        \n",
    "    if Distribution == \"Exponential\":\n",
    "        X_mn = np.linspace(0, 4, n_samples)\n",
    "        m = np.random.random_sample() * 25\n",
    "        c = np.random.random_sample() * 25\n",
    "        y_mn = m*np.exp(X_mn) + c + np.random.rand(*X_mn.shape) * noise\n",
    "        \n",
    "    if Distribution == \"Logrithmic\":\n",
    "        X_mn = np.linspace(0.5, 25, n_samples)\n",
    "        m = np.random.random_sample() * 25\n",
    "        c = np.random.random_sample() * 25\n",
    "        y_mn = m*np.log(X_mn) + c + np.random.rand(*X_mn.shape) * noise\n",
    "        \n",
    "    if Distribution == \"Blobs\":\n",
    "        for i in range(centers):\n",
    "            X_mn = np.random.rand(n_samples) * np.exp(noise)\n",
    "            y_mn = np.random.rand(n_samples) * np.exp(noise)\n",
    "            m = np.random.randint(low = -100, high = 100) * 25\n",
    "            c = np.random.randint(low = -100, high = 100) * 25\n",
    "            X_mn += m\n",
    "            y_mn += c\n",
    "            if i == 0:\n",
    "                con_X_mn = X_mn.copy()\n",
    "                con_y_mn = y_mn.copy()\n",
    "                \n",
    "            else:\n",
    "                con_X_mn = np.concatenate((con_X_mn, X_mn))\n",
    "                con_y_mn = np.concatenate((con_y_mn, y_mn))\n",
    "                \n",
    "        \n",
    "    X = X_mn\n",
    "    Y = y_mn\n",
    "    \n",
    "    X = X/np.max(X)\n",
    "    O = X\n",
    "    O_ = np.hstack((np.ones((X.shape[0],1)),np.c_[X]))\n",
    "    if(checkbox1==True):\n",
    "        r = LassoLarsCV(cv=5).fit(O_, Y)\n",
    "        Alpha = r.alpha_\n",
    "        print('Optimal Alpha = ' + str(Alpha))\n",
    "    reg = LassoLars(alpha=Alpha)\n",
    "    reg.fit(O_,Y)\n",
    "    theta = reg.coef_\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X,Y, s=10, label ='Datapoints')\n",
    "    y_pred = reg.predict(O_)\n",
    "    plt.plot(X, y_pred, color='r', label=\"fitted curve\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "#     plt.label()\n",
    "    plt.title(\"Lars Lasso Regression with alpha: \"+ str(Alpha))\n",
    "    plt.show()\n",
    "    #return X,Y, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lars Lasso \n",
    "\n",
    "Feature selection is often used to come up with better models with lower computations and better accuracy. Feature selection methods like Forward Selection and Backward Selection look into each of the features and try to add or delete features that are considered essential and less critical respectively. It comes in handy when dealing with high-dimensional data as it takes into consideration the correlation of features with the residual to select the active features. The ```LARS``` algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one's correlations with the residual. This process is iterated until we run out of all the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654a06df556243ac968e74e5623ce83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='iterations', max=20, min=1), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "def lars_path(iterations):\n",
    "    diabetes = datasets.load_diabetes()\n",
    "    X = diabetes.data\n",
    "    y = diabetes.target\n",
    "\n",
    "    _, _, coefs = linear_model.lars_path(X, y, method='lar', verbose=True,max_iter=iterations)\n",
    "\n",
    "    xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "    xx /= xx[-1]\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(xx, coefs.T)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "    plt.xlabel('|coef| / max|coef|')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title('LARS Path')\n",
    "    plt.axis('tight')\n",
    "    plt.show()\n",
    "    \n",
    "widget = interactive(lars_path,\n",
    "                     iterations = (1, 20, 1))\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```LARS LASSO (least absolute shrinkage and selection operator)``` is a regression method that performs both variable selection and regularization to reduce the model complexity for better generalizability. The ``LASSO`` modification to the ``LARS`` algorithm eliminates a particular feature from the active set when the corresponding coefficient nears zero. Thus ``LARS LASSO`` is a combination of forward and backward feature selection methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f2d3acc5f34558848b5056cc702984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Distribution', options=('Linear', 'Exponential', 'Logrithmic'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nbi:hide_in\n",
    "checkbox1 = widgets.Checkbox(description='Use Optimal Alpha')\n",
    "checkbox2 = widgets.Checkbox(description='Use Non-linear Data')\n",
    "widget = interactive(LarsLasso,\n",
    "                     Distribution=['Linear', 'Exponential','Logrithmic'],#,'Blobs'],\n",
    "                     noise = (0, 1e2, 1),\n",
    "                     n_samples = (10, 1e2, 10),\n",
    "                     LogAlpha = (-10, 10, 0.05),\n",
    "#                      seed = (0, 10, 1),\n",
    "                     checkbox1=checkbox1)\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abhvya Chandra | Apoorv Agnihotri | Jatin Dholakia | Sagar Gupta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
